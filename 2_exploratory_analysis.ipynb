{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f58977b8-76a9-4f27-a62a-5523bc2bb76e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44382324-1d24-401f-8859-a2eaae5ae9c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"uber_data_clean\")\n",
    "\n",
    "print(f\"✓ Records loaded: {df.count():,}\")\n",
    "print(f\"✓ Columns: {len(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "321c21d4-613c-451a-82ac-e5f5f7bdb3bb",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763085471569}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "overall_metrics = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as TotalBookings,\n",
    "    SUM(IsCompleted) as Completed_rides,\n",
    "    SUM(IsCustomerCancelled + IsDriverCancelled) as Cancelled_rides,\n",
    "    SUM(IsNoDriverFound + IsIncomplete) as Incomplete_rides,\n",
    "    ROUND(SUM(IsCompleted) / COUNT(*) * 100, 2) as SuccessRate_Percent,\n",
    "    ROUND(SUM(IsCustomerCancelled + IsDriverCancelled) / COUNT(*) * 100, 2) as CancellationRate_Percent,\n",
    "    ROUND(SUM(IsNoDriverFound + IsIncomplete) / COUNT(*) * 100, 2) as IncompleteRate_Percent\n",
    "FROM uber_data_clean\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n✓ OVERALL METRICS:\")\n",
    "overall_metrics.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6a9c866-3a01-4e04-a972-771a829e521c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vehicle_analysis = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    VehicleType,\n",
    "    COUNT(*) as TotalBookings,\n",
    "    SUM(IsCompleted) as CompletedBookings,\n",
    "    SUM(IsCustomerCancelled + IsDriverCancelled) as CancelledBookings,\n",
    "    SUM(IsCustomerCancelled) as CustomerCancellations,\n",
    "    SUM(IsDriverCancelled) as DriverCancellations,\n",
    "    SUM(IsNoDriverFound) as NoDriverFound,\n",
    "    SUM(IsIncomplete) as IncompleteRides,\n",
    "    ROUND(SUM(IsCustomerCancelled + IsDriverCancelled) / COUNT(*) * 100, 2) as CancellationRate_Percent,\n",
    "    ROUND(SUM(IsCompleted) / COUNT(*) * 100, 2) as CompletedRate_Percent,\n",
    "    ROUND(AVG(BookingValue), 2) as AvgBookingValue,\n",
    "    ROUND(SUM(BookingValue), 0) as TotalRevenue,\n",
    "    ROUND(AVG(DriverRating), 2) as AvgDriverRating,\n",
    "    ROUND(AVG(CustomerRating), 2) as AvgCustomerRating\n",
    "FROM uber_data_clean\n",
    "GROUP BY VehicleType\n",
    "ORDER BY CancellationRate_Percent DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n✓ CANCELLATION BY VEHICLE TYPE:\")\n",
    "vehicle_analysis.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52e21672-c866-4960-8e57-b18d51181abd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hourly_analysis = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    Hour,\n",
    "    COUNT(*) as TotalBookings,\n",
    "    SUM(IsCompleted) as CompletedBookings,\n",
    "    SUM(IsCustomerCancelled + IsDriverCancelled) as CancelledBookings,\n",
    "    SUM(IsCustomerCancelled) as CustomerCancellations,\n",
    "    SUM(IsDriverCancelled) as DriverCancellations,\n",
    "    SUM(IsNoDriverFound) as NoDriverFound,\n",
    "    ROUND(SUM(IsCustomerCancelled + IsDriverCancelled) / COUNT(*) * 100, 2) as CancellationRate_Percent,\n",
    "    ROUND(SUM(BookingValue), 0) as TotalRevenue,\n",
    "    ROUND(AVG(AvgVTAT), 2) as AvgWaitTime_Min\n",
    "FROM uber_data_clean\n",
    "WHERE Hour IS NOT NULL\n",
    "GROUP BY Hour\n",
    "ORDER BY Hour\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n✓ CANCELLATION BY HOUR:\")\n",
    "hourly_analysis.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90559851-022e-48bd-bcb6-a8c008d1807b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "daily_analysis = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    DayOfWeek,\n",
    "    DayOfWeekName,\n",
    "    IsWeekend,\n",
    "    COUNT(*) as TotalBookings,\n",
    "    SUM(IsCompleted) as CompletedBookings,\n",
    "    SUM(IsCustomerCancelled + IsDriverCancelled) as CancelledBookings,\n",
    "    ROUND(SUM(IsCustomerCancelled + IsDriverCancelled) / COUNT(*) * 100, 2) as CancellationRate_Percent,\n",
    "    ROUND(SUM(BookingValue), 0) as TotalRevenue,\n",
    "    ROUND(AVG(CustomerRating), 2) as AvgCustomerRating\n",
    "FROM uber_data_clean\n",
    "GROUP BY DayOfWeek, DayOfWeekName, IsWeekend\n",
    "ORDER BY DayOfWeek\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n✓ CANCELLATION BY DAY OF WEEK:\")\n",
    "daily_analysis.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81dbc9c6-d78f-4ee9-bc98-68a7d982eb2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "time_period_analysis = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    TimePeriod,\n",
    "    COUNT(*) as TotalBookings,\n",
    "    SUM(IsCompleted) as CompletedBookings,\n",
    "    SUM(IsCustomerCancelled + IsDriverCancelled) as CancelledBookings,\n",
    "    ROUND(SUM(IsCustomerCancelled + IsDriverCancelled) / COUNT(*) * 100, 2) as CancellationRate_Percent,\n",
    "    ROUND(AVG(AvgVTAT), 2) as AvgWaitTime_Min,\n",
    "    ROUND(AVG(BookingValue), 2) as AvgBookingValue,\n",
    "    ROUND(SUM(BookingValue), 0) as TotalRevenue\n",
    "FROM uber_data_clean\n",
    "GROUP BY TimePeriod\n",
    "ORDER BY CancellationRate_Percent DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n✓ CANCELLATION BY TIME PERIOD (Peak vs Off-Peak):\")\n",
    "time_period_analysis.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f5d8ee-4cf1-427f-b5ff-704b7400c3e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "customer_reasons = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    CustomerCancellationReason as CancellationReason,\n",
    "    COUNT(*) as Count,\n",
    "    ROUND(COUNT(*) / SUM(COUNT(*)) OVER() * 100, 2) as Percentage\n",
    "FROM uber_data_clean\n",
    "WHERE IsCustomerCancelled = 1 AND CustomerCancellationReason != 'Unknown'\n",
    "GROUP BY CustomerCancellationReason\n",
    "ORDER BY Count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n✓ TOP CUSTOMER CANCELLATION REASONS:\")\n",
    "customer_reasons.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d9a08cc-0b82-417f-aef1-c5e62cbd8a1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "driver_reasons = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    DriverCancellationReason as CancellationReason,\n",
    "    COUNT(*) as Count,\n",
    "    ROUND(COUNT(*) / SUM(COUNT(*)) OVER() * 100, 2) as Percentage\n",
    "FROM uber_data_clean\n",
    "WHERE IsDriverCancelled = 1 AND DriverCancellationReason != 'Unknown'\n",
    "GROUP BY DriverCancellationReason\n",
    "ORDER BY Count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n✓ TOP DRIVER CANCELLATION REASONS:\")\n",
    "driver_reasons.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2d04d36-fc7a-4cb6-9af9-fac5786e7ec4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "incomplete_reasons = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    IncompleteRideReason as IncompleteReason,\n",
    "    COUNT(*) as Count,\n",
    "    ROUND(COUNT(*) / SUM(COUNT(*)) OVER() * 100, 2) as Percentage\n",
    "FROM uber_data_clean\n",
    "WHERE IsIncomplete = 1 AND IncompleteRideReason != 'Unknown'\n",
    "GROUP BY IncompleteRideReason\n",
    "ORDER BY Count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n✓ TOP INCOMPLETE RIDE REASONS:\")\n",
    "incomplete_reasons.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34c33f3d-b531-41fa-acb6-c1c4c6ab81a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wait_time_analysis = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    CASE \n",
    "        WHEN AvgVTAT <= 2 THEN 'Under 2 min'\n",
    "        WHEN AvgVTAT <= 5 THEN '2-5 min'\n",
    "        WHEN AvgVTAT <= 10 THEN '5-10 min'\n",
    "        WHEN AvgVTAT <= 15 THEN '10-15 min'\n",
    "        ELSE 'Over 15 min'\n",
    "    END as WaitTimeBucket,\n",
    "    CASE \n",
    "        WHEN AvgVTAT <= 2 THEN 1\n",
    "        WHEN AvgVTAT <= 5 THEN 2\n",
    "        WHEN AvgVTAT <= 10 THEN 3\n",
    "        WHEN AvgVTAT <= 15 THEN 4\n",
    "        ELSE 5\n",
    "    END as SortOrder,\n",
    "    COUNT(*) as TotalBookings,\n",
    "    SUM(IsCompleted) as CompletedBookings,\n",
    "    SUM(IsCustomerCancelled + IsDriverCancelled) as CancelledBookings,\n",
    "    ROUND(SUM(IsCustomerCancelled + IsDriverCancelled) / COUNT(*) * 100, 2) as CancellationRate_Percent,\n",
    "    ROUND(AVG(BookingValue), 2) as AvgBookingValue\n",
    "FROM uber_data_clean\n",
    "WHERE AvgVTAT > 0\n",
    "GROUP BY \n",
    "    CASE \n",
    "        WHEN AvgVTAT <= 2 THEN 'Under 2 min'\n",
    "        WHEN AvgVTAT <= 5 THEN '2-5 min'\n",
    "        WHEN AvgVTAT <= 10 THEN '5-10 min'\n",
    "        WHEN AvgVTAT <= 15 THEN '10-15 min'\n",
    "        ELSE 'Over 15 min'\n",
    "    END,\n",
    "    CASE \n",
    "        WHEN AvgVTAT <= 2 THEN 1\n",
    "        WHEN AvgVTAT <= 5 THEN 2\n",
    "        WHEN AvgVTAT <= 10 THEN 3\n",
    "        WHEN AvgVTAT <= 15 THEN 4\n",
    "        ELSE 5\n",
    "    END\n",
    "ORDER BY SortOrder\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n✓ CANCELLATION BY WAIT TIME:\")\n",
    "wait_time_analysis.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2efa871-71e6-470a-917d-4ec9be6e495c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "monthly_trends = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    Month,\n",
    "    MonthName,\n",
    "    COUNT(*) as TotalBookings,\n",
    "    SUM(IsCompleted) as CompletedBookings,\n",
    "    SUM(IsCustomerCancelled + IsDriverCancelled) as CancelledBookings,\n",
    "    ROUND(SUM(IsCustomerCancelled + IsDriverCancelled) / COUNT(*) * 100, 2) as CancellationRate_Percent,\n",
    "    ROUND(SUM(BookingValue), 0) as TotalRevenue,\n",
    "    ROUND(AVG(CustomerRating), 2) as AvgCustomerRating\n",
    "FROM uber_data_clean\n",
    "GROUP BY Month, MonthName\n",
    "ORDER BY Month\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n✓ MONTHLY TRENDS:\")\n",
    "monthly_trends.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c5dc149-6181-4fb5-9e15-8ba391a0ae6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "insights = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    'Overall Success Rate' as InsightType,\n",
    "    CONCAT(ROUND(SUM(IsCompleted) / COUNT(*) * 100, 2), '%') as Value\n",
    "FROM uber_data_clean\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Overall Cancellation Rate' as InsightType,\n",
    "    CONCAT(ROUND(SUM(IsCustomerCancelled + IsDriverCancelled) / COUNT(*) * 100, 2), '%') as Value\n",
    "FROM uber_data_clean\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Most Common Cancellation Type' as InsightType,\n",
    "    (SELECT CONCAT(\n",
    "        CASE \n",
    "            WHEN SUM(IsCustomerCancelled) > SUM(IsDriverCancelled) THEN 'Customer'\n",
    "            ELSE 'Driver'\n",
    "        END\n",
    "    ) FROM uber_data_clean) as Value\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Highest Cancellation Rate Vehicle' as InsightType,\n",
    "    (SELECT VehicleType FROM (\n",
    "        SELECT VehicleType, ROUND(SUM(IsCustomerCancelled + IsDriverCancelled) / COUNT(*) * 100, 2) as rate\n",
    "        FROM uber_data_clean\n",
    "        GROUP BY VehicleType\n",
    "        ORDER BY rate DESC\n",
    "        LIMIT 1\n",
    "    )) as Value\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Lowest Cancellation Rate Vehicle' as InsightType,\n",
    "    (SELECT VehicleType FROM (\n",
    "        SELECT VehicleType, ROUND(SUM(IsCustomerCancelled + IsDriverCancelled) / COUNT(*) * 100, 2) as rate\n",
    "        FROM uber_data_clean\n",
    "        GROUP BY VehicleType\n",
    "        ORDER BY rate ASC\n",
    "        LIMIT 1\n",
    "    )) as Value\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Peak Cancellation Hour' as InsightType,\n",
    "    (SELECT CONCAT(Hour, ':00') FROM (\n",
    "        SELECT Hour, ROUND(SUM(IsCustomerCancelled + IsDriverCancelled) / COUNT(*) * 100, 2) as rate\n",
    "        FROM uber_data_clean\n",
    "        WHERE Hour IS NOT NULL\n",
    "        GROUP BY Hour\n",
    "        ORDER BY rate DESC\n",
    "        LIMIT 1\n",
    "    )) as Value\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Total Bookings' as InsightType,\n",
    "    CAST(COUNT(*) as STRING) as Value\n",
    "FROM uber_data_clean\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n✓ KEY INSIGHTS:\")\n",
    "insights.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5128267292050404,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2_exploratory_analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
